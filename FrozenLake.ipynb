{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/m1900l8x14q13k98_52xt6t40000gn/T/ipykernel_9755/2000219848.py:104: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor(states).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50/500, Reward: 0.0\n",
      "Episode 100/500, Reward: 0.16\n",
      "Episode 150/500, Reward: 0.1\n",
      "Episode 200/500, Reward: 0.28\n",
      "Episode 250/500, Reward: 0.66\n",
      "Episode 300/500, Reward: 0.74\n",
      "Episode 350/500, Reward: 0.7\n",
      "Episode 400/500, Reward: 0.9\n",
      "Episode 450/500, Reward: 0.88\n",
      "Episode 500/500, Reward: 0.96\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Hyperparameters\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.99  # Discount factor\n",
    "LR = 0.001  # Learning rate\n",
    "BATCH_SIZE = 64  # Batch size for training\n",
    "BUFFER_SIZE = 10000  # Replay buffer size\n",
    "EPSILON_START = 1.0  # Initial exploration rate\n",
    "EPSILON_END = 0.01  # Final exploration rate\n",
    "EPSILON_DECAY = 0.995  # Decay rate for epsilon\n",
    "TARGET_UPDATE = 10  # Frequency for updating target network\n",
    "EPISODES = 500  # Total episodes to train\n",
    "\n",
    "# Set device\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Neural Network for Q-function\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.ReLU(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Training DQN\n",
    "def train_dqn():\n",
    "    # Initialize environment and networks\n",
    "    env = gym.make(ENV_NAME, is_slippery=False)  # Deterministic mode\n",
    "    state_size = env.observation_space.n\n",
    "    action_size = env.action_space.n\n",
    "    q_network = QNetwork(state_size, action_size).to(device)\n",
    "    target_network = QNetwork(state_size, action_size).to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "    target_network.eval()\n",
    "\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=LR)\n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "    epsilon = EPSILON_START\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    # One-hot encoding for discrete states\n",
    "    def one_hot_encoding(state, state_size):\n",
    "        one_hot = np.zeros(state_size)\n",
    "        one_hot[state] = 1\n",
    "        return one_hot\n",
    "\n",
    "    # Training Loop\n",
    "    for episode in range(EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        state = one_hot_encoding(state, state_size)\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            # Select action using epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    action = torch.argmax(q_network(state_tensor)).item()\n",
    "\n",
    "            # Take action\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = one_hot_encoding(next_state, state_size)\n",
    "            replay_buffer.add((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Train Q-network\n",
    "            if len(replay_buffer) >= BATCH_SIZE:\n",
    "                experiences = replay_buffer.sample(BATCH_SIZE)\n",
    "                states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "                states = torch.FloatTensor(states).to(device)\n",
    "                actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "                rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "                next_states = torch.FloatTensor(next_states).to(device)\n",
    "                dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "                q_values = q_network(states).gather(1, actions)\n",
    "                next_q_values = target_network(next_states).max(1, keepdim=True)[0]\n",
    "                targets = rewards + (1 - dones) * GAMMA * next_q_values\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        # Update target network\n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            print(f\"Episode {episode + 1}/{EPISODES}, Reward: {np.mean(rewards_per_episode[-50:])}\")\n",
    "\n",
    "    return q_network\n",
    "\n",
    "\n",
    "q_network = train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def visualize_agent(env, q_network, state_size, delay=0.5):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    def one_hot_encoding(state, state_size):\n",
    "        one_hot = np.zeros(state_size)\n",
    "        one_hot[state] = 1\n",
    "        return one_hot\n",
    "\n",
    "    while not done:\n",
    "        env.render()  # Visualize the environment\n",
    "        state_tensor = torch.FloatTensor(one_hot_encoding(state, state_size)).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(q_network(state_tensor)).item()\n",
    "\n",
    "        next = env.step(action)\n",
    "        print(next)\n",
    "        state, reward, done, _, _ = next\n",
    "        total_reward += reward\n",
    "        time.sleep(delay)  # Pause for a short duration to simulate animation\n",
    "\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "    env.render()  # Display the final state\n",
    "\n",
    "\n",
    "# Visualize the trained agent\n",
    "env = gym.make(ENV_NAME, is_slippery=False, render_mode=\"human\")\n",
    "visualize_agent(env, q_network, env.observation_space.n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
