{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "transformations = [\n",
    "    lambda x: x,                         # Identity\n",
    "    lambda x: np.rot90(x, 1),            # Rotate 90°\n",
    "    lambda x: np.rot90(x, 2),            # Rotate 180°\n",
    "    lambda x: np.rot90(x, 3),            # Rotate 270°\n",
    "    lambda x: np.fliplr(x),              # Horizontal reflection\n",
    "    lambda x: np.flipud(x),              # Vertical reflection\n",
    "    lambda x: np.transpose(x),           # Diagonal reflection (TL-BR)\n",
    "    # lambda x: np.fliplr(np.transpose(x)) # Diagonal reflection (TR-BL)\n",
    "]\n",
    "\n",
    "inverse_transformations = [\n",
    "    lambda x: x,                         # Identity\n",
    "    lambda x: np.rot90(x, 3),            # Rotate 90° inverse (rotate 270°)\n",
    "    lambda x: np.rot90(x, 2),            # Rotate 180° inverse\n",
    "    lambda x: np.rot90(x, 1),            # Rotate 270° inverse (rotate 90°)\n",
    "    lambda x: np.fliplr(x),              # Horizontal reflection inverse\n",
    "    lambda x: np.flipud(x),              # Vertical reflection inverse\n",
    "    lambda x: np.transpose(x),           # Diagonal reflection (TL-BR) inverse\n",
    "    # lambda x: np.transpose(np.fliplr(x))# Diagonal reflection (TR-BL) inverse\n",
    "]\n",
    "\n",
    "original_actions = np.array(range(9)).reshape(3, 3)\n",
    "for i, transform in enumerate(transformations):\n",
    "    assert inverse_transformations[i](transform(original_actions)).flatten().tolist() == original_actions.flatten().tolist()\n",
    "\n",
    "transformed_actions = [transform(original_actions).flatten().tolist() for transform in transformations]\n",
    "for i in range(len(transformed_actions)):\n",
    "    for j in range(i + 1, len(transformed_actions)):\n",
    "        assert transformed_actions[i] != transformed_actions[j]\n",
    "\n",
    "def generate_all_valid_boards():\n",
    "    symbols = [' ', 'X', 'O']\n",
    "    all_boards = list(product(symbols, repeat=9))  # Generate all 3^9 combinations\n",
    "    print(f\"Total number of boards: {len(all_boards)}\")\n",
    "    all_valid_boards = []\n",
    "\n",
    "    for board in all_boards:\n",
    "        x_count = board.count('X')\n",
    "        o_count = board.count('O')\n",
    "        \n",
    "        # Valid boards must satisfy these conditions:\n",
    "        if x_count == o_count or x_count == o_count + 1:\n",
    "            all_valid_boards.append(board)\n",
    "\n",
    "    print(f\"Number of valid boards: {len(all_valid_boards)}\")    \n",
    "    return all_valid_boards\n",
    "\n",
    "def board_to_matrix(board):\n",
    "    return np.array(board).reshape(3, 3)\n",
    "\n",
    "def matrix_to_board(matrix):\n",
    "    return matrix.flatten().tolist()\n",
    "\n",
    "def generate_symmetries(board):\n",
    "    matrix = board_to_matrix(board)\n",
    "    symmetries = [transform(matrix) for transform in transformations]\n",
    "    return [matrix_to_board(sym) for sym in symmetries]\n",
    "\n",
    "def get_canonical_representation(board):\n",
    "    symmetries = generate_symmetries(board)\n",
    "    min_symmetry = min(symmetries)\n",
    "    return tuple(min_symmetry), symmetries.index(min_symmetry)\n",
    "\n",
    "# Generate all empty positions on the board\n",
    "def get_empty_positions(board):\n",
    "    return [i for i, cell in enumerate(board) if cell == ' ']\n",
    "\n",
    "# Generate all valid Tic-Tac-Toe boards\n",
    "all_valid_boards = generate_all_valid_boards()\n",
    "state_action_pairs = 0\n",
    "for valid_board in all_valid_boards:\n",
    "    empty_positions = get_empty_positions(valid_board)\n",
    "    state_action_pairs += len(empty_positions)\n",
    "\n",
    "print(f\"Total number of valid state-action pairs: {state_action_pairs}\")\n",
    "\n",
    "# Generate all canonical Tic-Tac-Toe boards\n",
    "all_canonical_boards = set()\n",
    "get_canonical_board = {}\n",
    "get_transform = {}\n",
    "get_inverse_transform = {}\n",
    "get_canonical_actions = {}\n",
    "get_inverse_canonical_actions = {}\n",
    "for valid_board in all_valid_boards:\n",
    "    canonical_board, transform_idx = get_canonical_representation(valid_board)\n",
    "    all_canonical_boards.add(canonical_board)\n",
    "    get_canonical_board[valid_board] = canonical_board\n",
    "    get_transform[valid_board] = transformations[transform_idx]\n",
    "    get_inverse_transform[valid_board] = inverse_transformations[transform_idx]\n",
    "    get_inverse_canonical_actions[valid_board] = get_transform[valid_board](original_actions).flatten().tolist()\n",
    "    get_canonical_actions[valid_board] = get_inverse_transform[valid_board](original_actions).flatten().tolist()\n",
    "\n",
    "all_canonical_boards = list(all_canonical_boards)\n",
    "print(f\"Number of canonical boards: {len(all_canonical_boards)}\")\n",
    "\n",
    "all_canonical_actions = {}\n",
    "canonical_state_action_pairs = 0\n",
    "for canonical_board in all_canonical_boards:\n",
    "    empty_positions = get_empty_positions(canonical_board)\n",
    "    all_canonical_actions[canonical_board] = empty_positions\n",
    "    canonical_state_action_pairs += len(empty_positions)\n",
    "\n",
    "print(f\"Number of canonical state-action pairs: {canonical_state_action_pairs}\")\n",
    "\n",
    "def get_canonical_action(board, action):\n",
    "    return get_canonical_actions[board][action]\n",
    "\n",
    "def get_inverse_canonical_action(board, canonical_action):\n",
    "    return get_inverse_canonical_actions[board][canonical_action]\n",
    "\n",
    "valid_board = all_valid_boards[5]\n",
    "actions = get_empty_positions(valid_board)\n",
    "canonical_actions = [get_canonical_action(valid_board, action) for action in actions]\n",
    "print(f\"Board:             {valid_board}\")\n",
    "print(f\"Canonical board:   {get_canonical_board[valid_board]}\")\n",
    "print(f\"Actions:           {actions}\")\n",
    "print(f\"Canonical actions: {sorted(canonical_actions)}\")\n",
    "print(f\"Canonical actions: {sorted(all_canonical_actions[get_canonical_board[valid_board]])}\")\n",
    "\n",
    "tot = 0\n",
    "for i, valid_board in enumerate(all_valid_boards):\n",
    "    actions = get_empty_positions(valid_board)\n",
    "    canonical_board = get_canonical_board[valid_board]\n",
    "    canonical_actions1 = sorted(all_canonical_actions[canonical_board])\n",
    "    canonical_actions2 = sorted([get_canonical_action(valid_board, action) for action in actions])\n",
    "    if not canonical_actions2 == canonical_actions1:\n",
    "        tot += 1\n",
    "\n",
    "print(f\"Number of wrong canonical actions: {tot}/{len(all_valid_boards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class SymmetricQMatrix:\n",
    "    def __init__(self, file=None, default_value=None):\n",
    "        self.default_value = 0.0\n",
    "        # Q-matrix storing canonical state-action pairs\n",
    "        if default_value is None and file is None:\n",
    "            self.q_matrix = defaultdict(self._initialize_q_matrix)\n",
    "        elif default_value is None and file is not None:\n",
    "            with open(file, 'rb') as f:\n",
    "                self.q_matrix = dill.load(f)\n",
    "\n",
    "        elif file is None and default_value is not None:\n",
    "            self.default_value = default_value\n",
    "            self.q_matrix = defaultdict(self._initialize_q_matrix)\n",
    "\n",
    "        # Precompute symmetries and their inverses\n",
    "        self.transformations = [\n",
    "            lambda x: x,                         # Identity\n",
    "            lambda x: np.rot90(x, 1),            # Rotate 90°\n",
    "            lambda x: np.rot90(x, 2),            # Rotate 180°\n",
    "            lambda x: np.rot90(x, 3),            # Rotate 270°\n",
    "            lambda x: np.fliplr(x),              # Horizontal reflection\n",
    "            lambda x: np.flipud(x),              # Vertical reflection\n",
    "            lambda x: np.transpose(x),           # Diagonal reflection (TL-BR)\n",
    "        ]\n",
    "\n",
    "        self.inverse_transformations = [\n",
    "            lambda x: x,                         # Identity\n",
    "            lambda x: np.rot90(x, 3),            # Rotate 90° inverse (rotate 270°)\n",
    "            lambda x: np.rot90(x, 2),            # Rotate 180° inverse\n",
    "            lambda x: np.rot90(x, 1),            # Rotate 270° inverse (rotate 90°)\n",
    "            lambda x: np.fliplr(x),              # Horizontal reflection inverse\n",
    "            lambda x: np.flipud(x),              # Vertical reflection inverse\n",
    "            lambda x: np.transpose(x),           # Diagonal reflection (TL-BR) inverse\n",
    "        ]\n",
    "\n",
    "        self.original_actions = np.array(range(9)).reshape(3, 3)\n",
    "        for i, transform in enumerate(self.transformations):\n",
    "            assert self.inverse_transformations[i](transform(self.original_actions)).flatten().tolist() == self.original_actions.flatten().tolist()\n",
    "\n",
    "        transformed_actions = [transform(original_actions).flatten().tolist() for transform in transformations]\n",
    "        for i in range(len(transformed_actions)):\n",
    "            for j in range(i + 1, len(transformed_actions)):\n",
    "                assert transformed_actions[i] != transformed_actions[j]\n",
    "\n",
    "        # Generate all valid Tic-Tac-Toe boards\n",
    "        self.all_valid_boards = self._generate_all_valid_boards()\n",
    "        self.all_canonical_boards = set()\n",
    "        self.get_canonical_boards = {}\n",
    "        self.get_transform = {}\n",
    "        self.get_inverse_transform = {}\n",
    "        self.get_canonical_actions = {}\n",
    "        self.get_inverse_canonical_actions = {}\n",
    "        for board in self.all_valid_boards:\n",
    "            canonical_board, transform_idx = self._get_canonical_representation(board)\n",
    "            self.all_canonical_boards.add(canonical_board)\n",
    "            self.get_canonical_boards[board] = canonical_board\n",
    "            self.get_transform[board] = self.transformations[transform_idx]\n",
    "            self.get_inverse_transform[board] = self.inverse_transformations[transform_idx]\n",
    "            self.get_inverse_canonical_actions[board] = self.get_transform[board](self.original_actions).flatten().tolist()\n",
    "            self.get_canonical_actions[board] = self.get_inverse_transform[board](self.original_actions).flatten().tolist()\n",
    "\n",
    "        self.all_canonical_boards = list(self.all_canonical_boards)\n",
    "        self.all_canonical_actions = {}\n",
    "        for board in self.all_canonical_boards:\n",
    "            empty_positions = self.get_empty_positions(board)\n",
    "            self.all_canonical_actions[board] = empty_positions\n",
    "\n",
    "    def _initialize_q_matrix(self):\n",
    "        state_dict = defaultdict(lambda: self.default_value)\n",
    "        return state_dict    \n",
    "\n",
    "    def _board_to_matrix(self, board):\n",
    "        \"\"\"\n",
    "        Convert a linear board to a 3x3 matrix for easier manipulation\n",
    "        \"\"\"\n",
    "        return np.array(board).reshape(3, 3)\n",
    "\n",
    "    def _matrix_to_board(self, matrix):\n",
    "        \"\"\"\n",
    "        Convert a 3x3 matrix back to a linear board representation.\n",
    "        \"\"\"\n",
    "        return matrix.flatten().tolist()\n",
    "\n",
    "    def _generate_all_valid_boards(self):\n",
    "        symbols = [' ', 'X', 'O']\n",
    "        all_boards = list(product(symbols, repeat=9))  # Generate all 3^9 combinations\n",
    "        # print(f\"Total number of boards: {len(all_boards)}\")\n",
    "        all_valid_boards = []\n",
    "\n",
    "        for board in all_boards:\n",
    "            x_count = board.count('X')\n",
    "            o_count = board.count('O')\n",
    "            \n",
    "            # Valid boards must satisfy these conditions:\n",
    "            if x_count == o_count or x_count == o_count + 1:\n",
    "                all_valid_boards.append(board)\n",
    "\n",
    "        # print(f\"Number of valid boards: {len(all_valid_boards)}\")    \n",
    "        return all_valid_boards\n",
    "\n",
    "    def _generate_symmetries(self, board):\n",
    "        matrix = self._board_to_matrix(board)\n",
    "        symmetries = [transform(matrix) for transform in self.transformations]\n",
    "        return [self._matrix_to_board(sym) for sym in symmetries]\n",
    "\n",
    "    def _get_canonical_representation(self, board):\n",
    "        symmetries = self._generate_symmetries(board)\n",
    "        min_symmetry = min(symmetries)\n",
    "        return tuple(min_symmetry), symmetries.index(min_symmetry)\n",
    "\n",
    "    def get_canonical_action(self, board, action):\n",
    "        return self.get_canonical_actions[tuple(board)][action]\n",
    "\n",
    "    def get_canonical_board(self, board):\n",
    "        return self.get_canonical_boards[tuple(board)]\n",
    "\n",
    "    def get_inverse_canonical_action(self, board, canonical_action):\n",
    "        return self.get_inverse_canonical_actions[tuple(board)][canonical_action]\n",
    "    \n",
    "    def canonicalize(self, board, action):\n",
    "        canonical_board = self.get_canonical_board(board)\n",
    "        canonical_action = self.get_canonical_action(board, action)\n",
    "        return canonical_board, canonical_action\n",
    "\n",
    "    # Generate all empty positions on the board\n",
    "    def get_empty_positions(self, board):\n",
    "        return [i for i, cell in enumerate(board) if cell == ' ']\n",
    "\n",
    "    def get(self, board=None, action=None):\n",
    "        \"\"\"\n",
    "        Retrieve the Q-value for a state-action pair.\n",
    "        \"\"\"\n",
    "        if board is None and action is None:\n",
    "            return self.q_matrix\n",
    "        if action is None and board is not None:\n",
    "            return self.q_matrix[self.get_canonical_board(board)]\n",
    "        if action is not None and board is not None:\n",
    "            canonical_board, canonical_action = self.canonicalize(board, action)\n",
    "            assert canonical_action in self.all_canonical_actions[canonical_board]\n",
    "            return self.q_matrix[canonical_board][canonical_action]\n",
    "\n",
    "    def set(self, board, action, value):\n",
    "        \"\"\"\n",
    "        Set the Q-value for a state-action pair.\n",
    "        \"\"\"\n",
    "        canonical_board, canonical_action = self.canonicalize(board, action)\n",
    "        assert canonical_action in self.all_canonical_actions[canonical_board]\n",
    "        self.q_matrix[canonical_board][canonical_action] = value\n",
    "\n",
    "    def best_actions(self, board, player='X'):\n",
    "        \"\"\"\n",
    "        Choose the best action based on Q-values for the current state.\n",
    "        \"\"\"\n",
    "        actions = self.get_empty_positions(board)\n",
    "\n",
    "        # Retrieve Q-values for all valid actions\n",
    "        q_values = {action: self.get(board, action) for action in actions}\n",
    "\n",
    "        # Choose based on player strategy\n",
    "        if player == 'X':\n",
    "            max_q = max(q_values.values())\n",
    "            best_actions = [action for action, q in q_values.items() if q == max_q]\n",
    "        else:\n",
    "            min_q = min(q_values.values())\n",
    "            best_actions = [action for action, q in q_values.items() if q == min_q]\n",
    "\n",
    "        return best_actions\n",
    "    \n",
    "    def best_action(self, board, player='X'):\n",
    "        \"\"\"\n",
    "        Choose the best action based on Q-values for the current state.\n",
    "        \"\"\"\n",
    "        best_actions = self.best_actions(board, player)\n",
    "        return np.random.choice(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "# random.seed(42)  # Set the random seed\n",
    "\n",
    "def zero_default():\n",
    "    return 0  # Replace with your desired default value\n",
    "\n",
    "# Initialize the Tic-Tac-Toe board\n",
    "def initialize_board():\n",
    "    return [' ' for _ in range(9)]\n",
    "\n",
    "# Generate all empty positions on the board\n",
    "def get_empty_positions(board):\n",
    "    return [i for i, cell in enumerate(board) if cell == ' ']\n",
    "\n",
    "# Display the board in a 3x3 format\n",
    "def display_board(board):\n",
    "    clear_output(wait=True)\n",
    "    print(\"\\n\")\n",
    "    print(f\" {board[0]}  |  {board[1]}  |  {board[2]} \")\n",
    "    print(\"----+-----+----\")\n",
    "    print(f\" {board[3]}  |  {board[4]}  |  {board[5]} \")\n",
    "    print(\"----+-----+----\")\n",
    "    print(f\" {board[6]}  |  {board[7]}  |  {board[8]} \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Display the board in a 3x3 format with Q-values on empty fields\n",
    "def display_board_with_Q(board, Q, Q_initial_value):\n",
    "    clear_output(wait=True)\n",
    "    field = {i: board[i] if board[i] != ' ' else Q.get(board, i) - Q_initial_value for i in range(9)}\n",
    "    def format_cell(value):\n",
    "        return f\"{value:.1f}\" if isinstance(value, (int, float)) else f\" {value} \"\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"{format_cell(field[0])} | {format_cell(field[1])} | {format_cell(field[2])}\")\n",
    "    print(\"----+-----+----\")\n",
    "    print(f\"{format_cell(field[3])} | {format_cell(field[4])} | {format_cell(field[5])}\")\n",
    "    print(\"----+-----+----\")\n",
    "    print(f\"{format_cell(field[6])} | {format_cell(field[7])} | {format_cell(field[8])}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Check for a winning condition\n",
    "def check_winner(board, player):\n",
    "    win_conditions = [\n",
    "        [0, 1, 2], [3, 4, 5], [6, 7, 8],  # rows\n",
    "        [0, 3, 6], [1, 4, 7], [2, 5, 8],  # columns\n",
    "        [0, 4, 8], [2, 4, 6]              # diagonals\n",
    "    ]\n",
    "    for condition in win_conditions:\n",
    "        if all(board[pos] == player for pos in condition):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Check for a draw (no empty spaces)\n",
    "def check_draw(board):\n",
    "    return ' ' not in board\n",
    "\n",
    "# Update Q-values based on the game's outcome, with correct max_future_q\n",
    "def update_q_values(Q, history, terminal_reward, alpha, gamma):\n",
    "    diff = 0\n",
    "    for i in range(len(history) - 1):\n",
    "        board, action = history[i]\n",
    "        next_board, _ = history[i + 1]\n",
    "        \n",
    "        # Calculate max Q-value for the next state over all possible actions\n",
    "        next_actions = get_empty_positions(next_board)\n",
    "        future_qs = [Q.get(next_board, next_action) for next_action in next_actions]\n",
    "        max_future_q = max(future_qs)\n",
    "        \n",
    "        # Update Q-value for current state-action pair\n",
    "        old_value = Q.get(board, action)\n",
    "        new_value = (1 - alpha) * old_value + alpha * gamma * max_future_q\n",
    "        Q.set(board, action, new_value)\n",
    "        diff += abs(old_value - new_value)\n",
    "\n",
    "    # Update the last state-action pair with the final reward\n",
    "    board, action = history[-1]\n",
    "    old_value = Q.get(board, action)\n",
    "    new_value = (1 - alpha) * old_value + alpha * terminal_reward\n",
    "    Q.set(board, action, new_value)\n",
    "    diff += abs(old_value - new_value)\n",
    "    return diff\n",
    "\n",
    "# Choose an action based on Q-values\n",
    "def choose_action(Q, board, player, epsilon=0.1):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        # Exploration: Choose a random move\n",
    "        empty_positions = get_empty_positions(board)\n",
    "        return random.choice(empty_positions)\n",
    "    else:\n",
    "        # Exploitation: Choose the best known move\n",
    "        return Q.best_action(board, player)\n",
    "\n",
    "# Main game loop for two random agents\n",
    "def play_game(matrices, params, flags):\n",
    "    if flags is None:\n",
    "        flags = {}\n",
    "\n",
    "    training = flags.get('training', True)\n",
    "    display = flags.get('display', False)\n",
    "    interactive = flags.get('interactive', False)\n",
    "    (Q, Visits, Rewards) = matrices\n",
    "    board = initialize_board()\n",
    "    current_player = 'X'\n",
    "    human_player = None\n",
    "    history = []  # To store state-action pairs\n",
    "    number_of_actions = 0\n",
    "    gamma = params['gamma']\n",
    "    episode = params['episode']\n",
    "    nr_of_episodes = params['nr_of_episodes']\n",
    "    epsilon = max(params['epsilon_min'], params['epsilon_start'] / (1 + episode/nr_of_episodes))\n",
    "    alpha = max(params['alpha_min'], params['alpha_start'] / (1 + episode/nr_of_episodes))\n",
    "\n",
    "    if interactive:\n",
    "        while human_player is None:\n",
    "            user_input = input(f\"Choose a player from the set {['X', 'O']}: \")\n",
    "            human_player = str(user_input)\n",
    "            if human_player not in ['X', 'O']:\n",
    "                human_player = None\n",
    "\n",
    "    while True:\n",
    "        if display:\n",
    "            display_board_with_Q(board, Q, params['Q_initial_value'])\n",
    "            time.sleep(params['waiting_time'])  # Wait a bit before the next move for readability\n",
    "\n",
    "        empty_positions = get_empty_positions(board)\n",
    "        if empty_positions:\n",
    "            if training:\n",
    "                action = choose_action(Q, board, current_player, epsilon=epsilon)\n",
    "                history.append((board[:], action))\n",
    "            else:\n",
    "                if interactive and current_player == human_player:\n",
    "                        action = None\n",
    "                        while action is None:\n",
    "                            user_input = input(f\"Choose a number from the set {empty_positions}: \")\n",
    "                            action = int(user_input)\n",
    "                            if action not in empty_positions:\n",
    "                                action = None\n",
    "                else:\n",
    "                    action = choose_action(Q, board, current_player, epsilon=params['eps'][current_player])\n",
    "\n",
    "            board[action] = current_player\n",
    "            number_of_actions += 1\n",
    "            \n",
    "            # Update Visits\n",
    "            Visits[tuple(board)][action] += 1\n",
    "\n",
    "        # Check for winner\n",
    "        if check_winner(board, current_player):\n",
    "            terminal_reward = 1.0 if current_player == 'X' else -1.0  # Reward for 'X', penalty for 'O'\n",
    "            if display:\n",
    "                display_board(board)\n",
    "                print(f\"Player {current_player} wins!\\n\")\n",
    "\n",
    "            if training:\n",
    "                update_q_values(Q, history, terminal_reward, alpha, gamma)\n",
    "\n",
    "            # Update Rewards\n",
    "            Rewards[tuple(board)][action] = terminal_reward\n",
    "\n",
    "            # Update outcomes\n",
    "            outcome = current_player\n",
    "            break\n",
    "        \n",
    "        # Check for draw\n",
    "        if check_draw(board):\n",
    "            terminal_reward = 0.5\n",
    "            if display:\n",
    "                display_board(board)\n",
    "                print(\"It's a draw!\\n\")\n",
    "\n",
    "            if training:\n",
    "                update_q_values(Q, history, terminal_reward, alpha, gamma)\n",
    "\n",
    "            # Update Rewards\n",
    "            Rewards[tuple(board)][action] = terminal_reward\n",
    "\n",
    "            # Update outcomes\n",
    "            outcome = 'D'\n",
    "            break\n",
    "        \n",
    "        # Switch players\n",
    "        current_player = 'O' if current_player == 'X' else 'X'\n",
    "\n",
    "    params['outcomes'][outcome] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the games begin\n",
    "# Parameters\n",
    "params = {\n",
    "    'Q_initial_value' : 1.0, # initial Q value\n",
    "    'alpha_start' : 0.1,  # initial learning rate\n",
    "    'alpha_min' : 0.1, # minimum learning rate\n",
    "    'gamma' : 0.9,  # discount factor\n",
    "    'epsilon_start' : 0.5,  # initial exploration rate\n",
    "    'epsilon_min' : 0.25, # minimum exploration rate\n",
    "    'waiting_time' : 1.0, # waiting in seconds for display\n",
    "    'episode' : 0, # number of episodes played during training\n",
    "    'outcomes' : {'X' : 0, 'O' : 0, 'D' : 0}, # dictionary to save the outcomes for games ('X' = X wins, 'O' = O wins, 'D' = draws) \n",
    "    'nr_of_episodes' : 500000, # number of episodes for training\n",
    "    'eps' : {'X' : -1.0, 'O' : -1.0}, # epsilon values for non-training determine how randomized the computer plays\n",
    "}\n",
    "\n",
    "# Initialize Q-matrix\n",
    "Q = SymmetricQMatrix(file='SymmetricQ.pkl')\n",
    "# Q = SymmetricQMatrix(default_value=params['Q_initial_value'])\n",
    "Visits = defaultdict(lambda: defaultdict(zero_default))\n",
    "Rewards = defaultdict(lambda: defaultdict(zero_default))\n",
    "\n",
    "matrices = (Q, Visits, Rewards)\n",
    "\n",
    "nr_of_episodes = 50000\n",
    "params['nr_of_episodes'] = nr_of_episodes\n",
    "params['episode'] = 0\n",
    "\n",
    "flags = {\n",
    "    'training': True,\n",
    "    'display': False,\n",
    "    'interactive': False\n",
    "}\n",
    "for _ in range(nr_of_episodes):\n",
    "    play_game(matrices, params, flags=flags)\n",
    "    params['episode'] += 1\n",
    "\n",
    "print(\"Outcomes:\")\n",
    "print(f\"X wins: {params['outcomes']['X']/nr_of_episodes}, O wins: {params['outcomes']['O']/nr_of_episodes}, draws: {params['outcomes']['D']/nr_of_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_episodes = 5000\n",
    "params['nr_of_episodes'] = nr_of_episodes\n",
    "params['eps'] = {'X' : 0.1, 'O' : -0.1}\n",
    "params['outcomes'] = {'X' : 0, 'O' : 0, 'D' : 0}\n",
    "flags = {\n",
    "    'training': False,\n",
    "    'display': False,\n",
    "    'interactive': False\n",
    "}\n",
    "for _ in range(nr_of_episodes):\n",
    "    play_game(matrices, params, flags=flags)\n",
    "\n",
    "print(\"Outcomes with players choosing action based on Q-values:\")\n",
    "print(f\"X wins: {params['outcomes']['X']/nr_of_episodes}, O wins: {params['outcomes']['O']/nr_of_episodes}, draws: {params['outcomes']['D']/nr_of_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcomes with players choosing action based on Q-values:\n",
    "X wins: 0.138, O wins: 0.6906, draws: 0.1714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "\n",
    "# with open('SymmetricQ.pkl', 'wb') as f:\n",
    "#     dill.dump(Q.get(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = {\n",
    "    'training': False,\n",
    "    'display': True,\n",
    "    'interactive': False\n",
    "}\n",
    "play_game(matrices, params, flags=flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = {\n",
    "    'training': False,\n",
    "    'display': True,\n",
    "    'interactive': True\n",
    "}\n",
    "# play_game(matrices, params, flags=flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_matrix = Q.get()\n",
    "for canonical_board in Q.all_canonical_boards:\n",
    "    setA = set(q_matrix[canonical_board].keys()) - set(Q.all_canonical_actions[canonical_board])\n",
    "    setB = set(Q.all_canonical_actions[canonical_board]) - set(q_matrix[canonical_board].keys())\n",
    "    if len(setA) > 0:\n",
    "        print(canonical_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q_matrix = Q.get()\n",
    "print(f\"Total unique states encountered: {len(q_matrix.keys())}\")\n",
    "\n",
    "# Extract all Q-values from the nested dictionary\n",
    "all_q_values = [q for actions in q_matrix.values() for q in actions.values()]\n",
    "print(f\"Total number of elements in Q: {len(all_q_values)}\")\n",
    "\n",
    "mean_q = np.mean(all_q_values)\n",
    "median_q = np.median(all_q_values)\n",
    "std_q = np.std(all_q_values)\n",
    "min_q = np.min(all_q_values)\n",
    "max_q = np.max(all_q_values)\n",
    "\n",
    "print(\"Q-value Statistics:\")\n",
    "print(f\"Mean: {mean_q}\")\n",
    "print(f\"Median: {median_q}\")\n",
    "print(f\"Standard Deviation: {std_q}\")\n",
    "print(f\"Minimum: {min_q}\")\n",
    "print(f\"Maximum: {max_q}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_q_values, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Histogram of Q-values\")\n",
    "plt.xlabel(\"Q-value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# with open('Q_optimal.pkl', 'wb') as f:\n",
    "#     dill.dump(Q, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of canonical boards: 1520\n",
    "Number of canonical state-action pairs: 4808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_v = [v for states in Visits.values() for v in states.values()]\n",
    "\n",
    "print(\"Statistics of visited state-action pairs:\")\n",
    "print(f\"Number of state-action pairs visited: {len(all_v)}\")\n",
    "\n",
    "mean_v = np.mean(all_v)\n",
    "median_v = np.median(all_v)\n",
    "std_v = np.std(all_v)\n",
    "min_v = np.min(all_v)\n",
    "max_v = np.max(all_v)\n",
    "\n",
    "print(f\"Mean: {mean_v}\")\n",
    "print(f\"Median: {median_v}\")\n",
    "print(f\"Standard Deviation: {std_v}\")\n",
    "print(f\"Minimum: {min_v}\")\n",
    "print(f\"Maximum: {max_v}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_v, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Histogram of visited states\")\n",
    "plt.ylabel(\"Visits\")\n",
    "plt.xlabel(\"States\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
