{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcomes during learning:\n",
      "X wins: 0.117871, O wins: 0.057712, draws: 0.824417\n",
      "Outcomes during playing:\n",
      "X wins: 0.9887, O wins: 0.0, draws: 0.0113\n",
      "Outcomes during playing:\n",
      "X wins: 0.0, O wins: 0.878, draws: 0.122\n",
      "Outcomes during playing:\n",
      "X wins: 0.0, O wins: 0.0, draws: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Let the games begin\n",
    "\n",
    "from TicTacToe import TicTacToe\n",
    "from Agent import RandomAgent, HumanAgent, LearningAgent, PlayingAgent\n",
    "\n",
    "params = {\n",
    "    'Q_initial_value' : 0.0, # initial Q value\n",
    "    # 'Q_initial_value' : 0.0, # initial Q value\n",
    "    'nr_of_episodes' : 500000, # number of episodes for training\n",
    "    'epsilon_start' : 0.1,  # initial exploration rate\n",
    "    'epsilon_min' : 0.005, # minimum exploration rate\n",
    "    # 'epsilon_start' : 0.5,  # initial exploration rate\n",
    "    # 'epsilon_min' : 0.25, # minimum exploration rate\n",
    "    'alpha_start' : 0.1,  # initial learning rate\n",
    "    'alpha_min' : 0.1, # minimum learning rate\n",
    "    # 'gamma' : 0.99,  # discount factor\n",
    "    'gamma' : 0.9,  # discount factor\n",
    "    # 'rewards' : {'X': {'X' : 1.0, 'O' : -1.0, 'D' : 0.0}, # rewards for X\n",
    "    #              'O': {'O' : 1.0, 'X' : -1.0, 'D' : 0.0}, # rewards for O\n",
    "    #             },\n",
    "    'rewards' : {'X': {'X' : 1.0, 'O' : -1.0, 'D' : 0.0}, # rewards for X\n",
    "                 'O': {'O' : 1.0, 'X' : -1.0, 'D' : 0.0}, # rewards for O\n",
    "                },\n",
    "    'switching' : False,\n",
    "    'debug' : False,\n",
    "    }\n",
    "paramsX = params.copy()\n",
    "paramsO = params.copy()\n",
    "paramsX['player'] = 'X'\n",
    "paramsO['player'] = 'O'\n",
    "\n",
    "nr_of_episodes = 1000000\n",
    "params['nr_of_episodes'] = nr_of_episodes\n",
    "outcomes = {'X' : 0, 'O' : 0, 'D' : 0}\n",
    "\n",
    "learning_agent1 = LearningAgent(paramsX)\n",
    "learning_agent2 = LearningAgent(paramsO)\n",
    "random_agent1 = RandomAgent(player='O', switching=False)\n",
    "\n",
    "# game = TicTacToe(learning_agent1, random_agent1, display=False)\n",
    "game = TicTacToe(learning_agent1, learning_agent2, display=False)\n",
    "\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during learning:\")\n",
    "print(f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\")\n",
    "Q1 = learning_agent1.Q\n",
    "playing_agent1 = PlayingAgent(Q1, player='X', switching=False)\n",
    "random_agent1 = RandomAgent(player='O', switching=False)\n",
    "\n",
    "game = TicTacToe(playing_agent1, random_agent1, display=False)\n",
    "nr_of_episodes = 10000\n",
    "outcomes = {'X' : 0, 'O' : 0, 'D' : 0}\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during playing:\")\n",
    "print(f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\")\n",
    "\n",
    "Q2 = learning_agent2.Q\n",
    "playing_agent2 = PlayingAgent(Q2, player='O', switching=False)\n",
    "random_agent2 = RandomAgent(player='X', switching=False)\n",
    "\n",
    "game = TicTacToe(playing_agent2, random_agent2, display=False)\n",
    "nr_of_episodes = 10000\n",
    "outcomes = {'X' : 0, 'O' : 0, 'D' : 0}\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during playing:\")\n",
    "print(f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\")\n",
    "\n",
    "game = TicTacToe(playing_agent1, playing_agent2, display=False)\n",
    "nr_of_episodes = 10000\n",
    "outcomes = {'X' : 0, 'O' : 0, 'D' : 0}\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during playing:\")\n",
    "print(f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X wins: 0.9941, O wins: 0.0, draws: 0.0059\n",
    "X wins: 0.9801, O wins: 0.0068, draws: 0.0131\n",
    "X wins: 0.9799, O wins: 0.0077, draws: 0.0124\n",
    "X wins: 0.9829, O wins: 0.0034, draws: 0.0137\n",
    "X wins: 0.9765, O wins: 0.0053, draws: 0.0182\n",
    "X wins: 0.9597, O wins: 0.0184, draws: 0.0219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.00 |  0.00 |  0.00\n",
      "---------------------\n",
      " 0.00 |  0.04 |  0.00\n",
      "---------------------\n",
      " 0.00 |  0.00 |  0.00\n",
      "\n",
      "\n",
      " 0.00 |  0.03 |  0.00\n",
      "---------------------\n",
      " 0.00 |   X   |  0.03\n",
      "---------------------\n",
      "  O   |  0.00 |  0.00\n",
      "\n",
      "\n",
      "-1.00 |   X   | -1.00\n",
      "---------------------\n",
      "-0.99 |   X   | -1.00\n",
      "---------------------\n",
      "  O   |   O   |  0.04\n",
      "\n",
      "\n",
      "  O   |   X   | -1.00\n",
      "---------------------\n",
      " 0.03 |   X   | -1.00\n",
      "---------------------\n",
      "  O   |   O   |   X  \n",
      "\n",
      "\n",
      "  O   |   X   |  0.00\n",
      "---------------------\n",
      "  X   |   X   |   O  \n",
      "---------------------\n",
      "  O   |   O   |   X  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 0.05 | -0.71 |  0.05\n",
      "---------------------\n",
      "-0.71 |   X   | -0.71\n",
      "---------------------\n",
      " 0.05 | -0.71 |  0.05\n",
      "\n",
      "\n",
      "-1.00 |   X   | -1.00\n",
      "---------------------\n",
      "-1.00 |   X   | -0.97\n",
      "---------------------\n",
      "  O   |  0.05 | -0.99\n",
      "\n",
      "\n",
      " 0.04 |   X   | -0.99\n",
      "---------------------\n",
      "-0.93 |   X   | -1.00\n",
      "---------------------\n",
      "  O   |   O   |   X  \n",
      "\n",
      "\n",
      "  O   |   X   | -1.00\n",
      "---------------------\n",
      "  X   |   X   |  0.00\n",
      "---------------------\n",
      "  O   |   O   |   X  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from playGame import get_empty_positions\n",
    "\n",
    "def displayQ(Q, board):\n",
    "    actions = get_empty_positions(board)  # Assume this function returns indices of empty positions\n",
    "    Qs = {action: f\"{Q.get(tuple(board), action):.2f}\" for action in actions}  # Get Q-values, default to 0\n",
    "    board_size = int(len(board) ** 0.5)  # Assume square board\n",
    "\n",
    "    # Create a new board layout with Q-values embedded\n",
    "    Qboard = list(board)\n",
    "    for action, value in Qs.items():\n",
    "        Qboard[action] = value  # Replace empty spots with Q-values\n",
    "\n",
    "    cell_width = 5  # Padding for centering\n",
    "\n",
    "    # Format and display the board\n",
    "    for i in range(board_size):\n",
    "        row = Qboard[i * board_size:(i + 1) * board_size]\n",
    "        formatted_row = \" | \".join(str(cell).center(cell_width) for cell in row)\n",
    "        print(formatted_row)\n",
    "        if i < board_size - 1:\n",
    "            print(\"-\" * (board_size * cell_width + (board_size - 1) * 3))  # Line separator   \n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "def display_history(Q, history):\n",
    "    for i in range(len(history)):\n",
    "        board, action = history[i]\n",
    "        displayQ(Q, board)\n",
    "\n",
    "Q1 = learning_agent1.Q\n",
    "historyX = paramsX['history']\n",
    "display_history(Q1, historyX)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "Q2 = learning_agent2.Q\n",
    "historyO = paramsO['history']\n",
    "display_history(Q2, historyO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "\n",
    "# with open('SymmetricQ.pkl', 'wb') as f:\n",
    "#     dill.dump(Q.get(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_outcomes = {'X' : 91/138, 'O' : 44/138, 'D' : 3/138}\n",
    "print(random_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = (' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
    "Q1 = learning_agent1.Q\n",
    "print(Q1.get(board).values())\n",
    "# Q2 = agent_2.Q\n",
    "# print(Q2.get(board).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SymmetricMatrix import TotallySymmetricMatrix, QTotallySymmetricMatrix\n",
    "\n",
    "board = (' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
    "Q1 = QTotallySymmetricMatrix(default_value=0.0)\n",
    "Q1.set(board, 0, 1.0)\n",
    "displayQ(Q1, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q1.get(initialize_board(), 0))\n",
    "print(Q1.get(initialize_board(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q_matrix = Q1.get()\n",
    "# print(f\"Total unique states encountered: {len(q_matrix.keys())}\")\n",
    "\n",
    "print(Q1.get(initialize_board()))\n",
    "\n",
    "# Extract all Q-values from the nested dictionary\n",
    "# all_q_values = [q for actions in q_matrix.values() for q in actions.values()]\n",
    "all_q_values = [q for q in q_matrix.values()]\n",
    "print(f\"Total number of elements in Q: {len(all_q_values)}\")\n",
    "\n",
    "mean_q = np.mean(all_q_values)\n",
    "median_q = np.median(all_q_values)\n",
    "std_q = np.std(all_q_values)\n",
    "min_q = np.min(all_q_values)\n",
    "max_q = np.max(all_q_values)\n",
    "\n",
    "print(\"Q-value Statistics:\")\n",
    "print(f\"Mean: {mean_q}\")\n",
    "print(f\"Median: {median_q}\")\n",
    "print(f\"Standard Deviation: {std_q}\")\n",
    "print(f\"Minimum: {min_q}\")\n",
    "print(f\"Maximum: {max_q}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_q_values, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Histogram of Q-values\")\n",
    "plt.xlabel(\"Q-value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of canonical boards: 1520\n",
    "Number of canonical state-action pairs: 4808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = Visits.get()\n",
    "# all_v = [v for states in visits.values() for v in states.values()]\n",
    "all_v = [q for q in visits.values()]\n",
    "\n",
    "print(\"Statistics of visited state-action pairs:\")\n",
    "print(f\"Number of state-action pairs visited: {len(all_v)}\")\n",
    "\n",
    "mean_v = np.mean(all_v)\n",
    "median_v = np.median(all_v)\n",
    "std_v = np.std(all_v)\n",
    "min_v = np.min(all_v)\n",
    "max_v = np.max(all_v)\n",
    "\n",
    "print(f\"Mean: {mean_v}\")\n",
    "print(f\"Median: {median_v}\")\n",
    "print(f\"Standard Deviation: {std_v}\")\n",
    "print(f\"Minimum: {min_v}\")\n",
    "print(f\"Maximum: {max_v}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_v, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Histogram of visited states\")\n",
    "plt.xlabel(\"Visits\")\n",
    "plt.ylabel(\"States\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = Rewards.get()\n",
    "\n",
    "# all_r = [r for states in rewards.values() for r in states.values()]\n",
    "# all_rX = [r for states in rewards.values() for r in states.values() if r == 1.0]\n",
    "# all_rO = [r for states in rewards.values() for r in states.values() if r == -1.0]\n",
    "# all_rD = [r for states in rewards.values() for r in states.values() if r == 0.5]\n",
    "\n",
    "all_r = [r for r in rewards.values()]\n",
    "all_rX = [r for r in rewards.values() if r == 1.0]\n",
    "all_rO = [r for r in rewards.values() if r == -1.0]\n",
    "all_rD = [r for r in rewards.values() if r == 0.5]\n",
    "\n",
    "print(\"Statistics of rewards\")\n",
    "print(f\"Number of state-action pairs with rewards: {len(all_r)}\")\n",
    "print(f\"Number of state-action pairs with rewards for X: {len(all_rX)}\")\n",
    "print(f\"Number of state-action pairs with rewards for O: {len(all_rO)}\")\n",
    "print(f\"Number of state-action pairs with rewards for D: {len(all_rD)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
