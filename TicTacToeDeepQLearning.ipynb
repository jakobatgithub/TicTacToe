{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the games begin\n",
    "\n",
    "import copy\n",
    "\n",
    "import wandb\n",
    "from Agent import RandomAgent\n",
    "from DeepQAgent import DeepQLearningAgent, DeepQPlayingAgent\n",
    "\n",
    "# os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "from TicTacToe import TicTacToe\n",
    "\n",
    "params = {\n",
    "    \"nr_of_episodes\": 500000,  # number of episodes for training\n",
    "    \"rows\": 3,  # rows of the board, rows = cols\n",
    "    \"epsilon_start\": 0.15,  # initial exploration rate\n",
    "    \"epsilon_min\": 0.005,  # minimum exploration rate\n",
    "    \"learning_rate\": 0.0001,  # learning rate\n",
    "    \"gamma\": 0.95,  # discount factor\n",
    "    \"switching\": True,  # switch between X and O\n",
    "    \"debug\": False,  # print debug messages\n",
    "    # Parameters for DeepQAgent\n",
    "    \"batch_size\": 128,  # batch size for deep learning\n",
    "    \"target_update_frequency\": 20,  # target network update frequency\n",
    "    \"evaluation\": True,  # save data for evaluation\n",
    "    \"double_q_learning\": False,  # flag to switch on double Q-learnning\n",
    "    \"device\": \"cpu\",  # device to use, 'cpu' or 'mps' or 'cuda'\n",
    "    \"replay_buffer_length\": 10000,  # replay buffer length\n",
    "    \"wandb_logging_frequency\": 50,  # wandb logging frequency\n",
    "}\n",
    "\n",
    "rows = 4\n",
    "win_length = 4\n",
    "# nr_of_episodes = 750000\n",
    "nr_of_episodes = 15000\n",
    "params[\"nr_of_episodes\"] = nr_of_episodes\n",
    "params[\"rows\"] = rows\n",
    "\n",
    "paramsX = copy.deepcopy(params)\n",
    "paramsO = copy.deepcopy(params)\n",
    "paramsX[\"player\"] = \"X\"\n",
    "paramsO[\"player\"] = \"O\"\n",
    "\n",
    "outcomes = {\"X\": 0, \"O\": 0, \"D\": 0}\n",
    "\n",
    "learning_agent1 = DeepQLearningAgent(paramsX)\n",
    "learning_agent2 = DeepQLearningAgent(paramsO)\n",
    "# random_agent2 = RandomAgent(player='O', switching=False)\n",
    "\n",
    "# game = TicTacToe(learning_agent1, random_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "game = TicTacToe(learning_agent1, learning_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during learning:\")\n",
    "print(\n",
    "    f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\"\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from DeepQAgent import ReplayBuffer\n",
    "\n",
    "board_to_state_translation = {\"X\": 1, \"O\": -1, \" \": 0}\n",
    "state_to_board_translation = {1: \"X\", -1: \"O\", 0: \" \"}\n",
    "\n",
    "\n",
    "def board_to_state(board):\n",
    "    return np.array([board_to_state_translation[cell] for cell in board]).reshape(1, -1)\n",
    "\n",
    "\n",
    "board_to_state([\" \"] * 9)\n",
    "\n",
    "rows = 3\n",
    "replay_buffer = ReplayBuffer(10000, rows**2, rows**2, device=\"cpu\")\n",
    "replay_buffer.add(board_to_state([\" \"] * 9), [0], 1.0, board_to_state([\" \"] * 9), False)\n",
    "\n",
    "len(replay_buffer)\n",
    "\n",
    "test_bool = torch.zeros(3, dtype=torch.bool)\n",
    "~test_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network1 = learning_agent1.q_network\n",
    "playing_agent1 = DeepQPlayingAgent(q_network1, player=\"X\", switching=False)\n",
    "random_agent2 = RandomAgent(player=\"O\", switching=False)\n",
    "\n",
    "game = TicTacToe(playing_agent1, random_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "nr_of_episodes = 1000\n",
    "outcomes = {\"X\": 0, \"O\": 0, \"D\": 0}\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during playing:\")\n",
    "print(\n",
    "    f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\"\n",
    ")\n",
    "\n",
    "q_network2 = learning_agent2.q_network\n",
    "playing_agent2 = DeepQPlayingAgent(q_network2, player=\"O\", switching=False)\n",
    "random_agent1 = RandomAgent(player=\"X\", switching=False)\n",
    "\n",
    "game = TicTacToe(random_agent1, playing_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "nr_of_episodes = 1000\n",
    "outcomes = {\"X\": 0, \"O\": 0, \"D\": 0}\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during playing:\")\n",
    "print(\n",
    "    f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\"\n",
    ")\n",
    "\n",
    "game = TicTacToe(playing_agent1, playing_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "nr_of_episodes = 1000\n",
    "outcomes = {\"X\": 0, \"O\": 0, \"D\": 0}\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during playing:\")\n",
    "print(\n",
    "    f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# torch.save(q_network1, 'models/q_network_4x4x4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import plot_evaluation_data, plot_valid_actions\n",
    "\n",
    "plot_evaluation_data(learning_agent1)\n",
    "plot_evaluation_data(learning_agent2)\n",
    "\n",
    "plot_valid_actions(learning_agent1)\n",
    "plot_valid_actions(learning_agent2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
