{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the games begin\n",
    "\n",
    "import copy\n",
    "\n",
    "from TicTacToe import TicTacToe\n",
    "from Agent import RandomAgent, HumanAgent\n",
    "from QAgent import QLearningAgent, QPlayingAgent\n",
    "from DeepQAgent import DeepQLearningAgent, DeepQPlayingAgent\n",
    "\n",
    "params = {\n",
    "    'nr_of_episodes' : 500000, # number of episodes for training\n",
    "    'epsilon_start' : 0.15,  # initial exploration rate\n",
    "    'epsilon_min' : 0.005, # minimum exploration rate\n",
    "    'alpha_start' : 0.1,  # initial learning rate\n",
    "    'alpha_min' : 0.1, # minimum learning rate\n",
    "    'gamma' : 0.9,  # discount factor\n",
    "    'switching' : False, # switch between X and O\n",
    "    'debug' : False, # print debug messages\n",
    "    'width' : 3, # width = height of the board\n",
    "    'evaluation' : True, # save data for evaluation\n",
    "\n",
    "    # Parameters for QAgent\n",
    "    'lazy_evaluation' : True, # use lazy evaluation\n",
    "    'Q_initial_value' : 0.0, # initial Q value\n",
    "\n",
    "    # Parameters for DeepQAgent\n",
    "    'batch_size' : 32, # batch size for deep learning\n",
    "    'double_q_learning' : True, # flag to switch on double Q-learnning\n",
    "    # 'target_update_frequency' : 500, # target network update frequency\n",
    "    'target_update_frequency' : 50, # target network update frequency\n",
    "    }\n",
    "\n",
    "nr_of_episodes = 250\n",
    "# nr_of_episodes = 1500\n",
    "params['nr_of_episodes'] = nr_of_episodes\n",
    "\n",
    "paramsX = copy.deepcopy(params)\n",
    "paramsO = copy.deepcopy(params)\n",
    "paramsX['player'] = 'X'\n",
    "paramsX['Q_optimal'] = 'TotallySymmetricQ_optimalX.pkl'\n",
    "paramsO['player'] = 'O'\n",
    "paramsO['Q_optimal'] = 'TotallySymmetricQ_optimalO.pkl'\n",
    "\n",
    "outcomes = {'X' : 0, 'O' : 0, 'D' : 0}\n",
    "\n",
    "learning_agent1 = DeepQLearningAgent(paramsX)\n",
    "# learning_agent2 = DeepQLearningAgent(paramsO)\n",
    "random_agent1 = RandomAgent(player='O', switching=False)\n",
    "\n",
    "game = TicTacToe(learning_agent1, random_agent1, display=False)\n",
    "# game = TicTacToe(learning_agent1, learning_agent2, display=False)\n",
    "\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during learning:\")\n",
    "print(f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qmodel = learning_agent1.Qmodel\n",
    "playing_agent1 = DeepQPlayingAgent(Qmodel, player='X', switching=False)\n",
    "random_agent1 = RandomAgent(player='O', switching=False)\n",
    "\n",
    "game = TicTacToe(playing_agent1, random_agent1, display=False)\n",
    "nr_of_episodes = 250\n",
    "outcomes = {'X' : 0, 'O' : 0, 'D' : 0}\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during playing:\")\n",
    "print(f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import plot_graphs\n",
    "\n",
    "evaluation_data = learning_agent1.evaluation_data\n",
    "lossX = evaluation_data['loss']\n",
    "avg_action_valueX = evaluation_data['avg_action_value']\n",
    "rewards = evaluation_data['rewards']\n",
    "print(f\"Number of losses: {len(lossX)}\")\n",
    "print(f\"Number of action values: {len(avg_action_valueX)}\")\n",
    "print(f\"Number of rewards: {len(rewards)}\")\n",
    "plot_graphs(lossX, avg_action_valueX, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import plot_graphs\n",
    "\n",
    "evaluation_data = learning_agent2.evaluation_data\n",
    "lossX = evaluation_data['loss']\n",
    "avg_action_valueX = evaluation_data['avg_action_value']\n",
    "rewards = evaluation_data['rewards']\n",
    "print(f\"Number of losses: {len(lossX)}\")\n",
    "print(f\"Number of action values: {len(avg_action_valueX)}\")\n",
    "print(f\"Number of rewards: {len(rewards)}\")\n",
    "plot_graphs(lossX, avg_action_valueX, rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
