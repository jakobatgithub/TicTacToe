{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the games begin\n",
    "\n",
    "import copy\n",
    "\n",
    "from TicTacToe import TicTacToe\n",
    "from Agent import RandomAgent, HumanAgent\n",
    "from QAgent import QLearningAgent, QPlayingAgent\n",
    "from DeepQAgent import DeepQLearningAgent, DeepQPlayingAgent\n",
    "\n",
    "params = {\n",
    "    'nr_of_episodes' : 500000, # number of episodes for training\n",
    "    'epsilon_start' : 0.15,  # initial exploration rate\n",
    "    'epsilon_min' : 0.005, # minimum exploration rate\n",
    "    'alpha_start' : 0.1,  # initial learning rate\n",
    "    'alpha_min' : 0.1, # minimum learning rate\n",
    "    'gamma' : 0.9,  # discount factor\n",
    "    'switching' : False, # switch between X and O\n",
    "    'debug' : False, # print debug messages\n",
    "    'width' : 5, # width = height of the board\n",
    "    'evaluation' : True, # save data for evaluation\n",
    "\n",
    "    # Parameters for QAgent\n",
    "    'lazy_evaluation' : True, # use lazy evaluation\n",
    "    'Q_initial_value' : 0.0, # initial Q value\n",
    "\n",
    "    # Parameters for DeepQAgent\n",
    "    'batch_size' : 32, # batch size for deep learning\n",
    "    # 'target_update_frequency' : 250, # target network update frequency\n",
    "    'target_update_frequency' : 150, # target network update frequency\n",
    "    }\n",
    "\n",
    "nr_of_episodes = 150000\n",
    "params['nr_of_episodes'] = nr_of_episodes\n",
    "\n",
    "paramsX = copy.deepcopy(params)\n",
    "paramsO = copy.deepcopy(params)\n",
    "paramsX['player'] = 'X'\n",
    "paramsO['player'] = 'O'\n",
    "\n",
    "outcomes = {'X' : 0, 'O' : 0, 'D' : 0}\n",
    "\n",
    "learning_agent1 = QLearningAgent(paramsX)\n",
    "learning_agent2 = QLearningAgent(paramsO)\n",
    "random_agent1 = RandomAgent(player='X', switching=False)\n",
    "random_agent2 = RandomAgent(player='O', switching=False)\n",
    "\n",
    "# game = TicTacToe(random_agent1, random_agent2, display=False, width=5, height=5, win_length=3)\n",
    "# game = TicTacToe(learning_agent1, random_agent2, display=False, width=5, height=5, win_length=3)\n",
    "# game = TicTacToe(random_agent1, learning_agent2, display=False, width=5, height=5, win_length=3)\n",
    "game = TicTacToe(learning_agent1, learning_agent2, display=False, width=5, height=5, win_length=3)\n",
    "\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during learning:\")\n",
    "print(f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import QAgent_plays_against_RandomAgent, QAgent_plays_against_QAgent\n",
    "\n",
    "Q1 = learning_agent1.Q\n",
    "Q2 = learning_agent2.Q\n",
    "\n",
    "QAgent_plays_against_RandomAgent(Q1, 'X', 2000, width=5, height=5, win_length=3)\n",
    "QAgent_plays_against_RandomAgent(Q2, 'O', 2000, width=5, height=5, win_length=3)\n",
    "QAgent_plays_against_QAgent(Q1, 'X', Q2, 'O', 2000, width=5, height=5, win_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import plot_evaluation_data\n",
    "\n",
    "plot_evaluation_data(learning_agent1)\n",
    "plot_evaluation_data(learning_agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "\n",
    "# with open('SymmetricQ_optimalX.pkl', 'wb') as f:\n",
    "#     dill.dump(Q1.get(), f)\n",
    "\n",
    "# with open('SymmetricQ_optimalO.pkl', 'wb') as f:\n",
    "#     dill.dump(Q2.get(), f)\n",
    "\n",
    "# with open('TotallySymmetricQ_optimalX.pkl', 'wb') as f:\n",
    "#     dill.dump(Q1.get(), f)\n",
    "\n",
    "# with open('TotallySymmetricQ_optimalO.pkl', 'wb') as f:\n",
    "#     dill.dump(Q2.get(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "k = 3\n",
    "outcomes = {'X' : 0, 'O' : 0, 'D' : 0}\n",
    "random_agent1 = RandomAgent(player='X', switching=False)\n",
    "random_agent2 = RandomAgent(player='O', switching=False)\n",
    "game = TicTacToe(random_agent1, random_agent2, display=False, waiting_time=0.05, width=m, height=m, win_length=k)\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes for random playing:\")\n",
    "print(f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\")\n",
    "\n",
    "paramsX['width'] = m\n",
    "paramsX['Q_optimal'] = None\n",
    "\n",
    "learning_agent = QLearningAgent(paramsX)\n",
    "\n",
    "outcomes = {'X' : 0, 'O' : 0, 'D' : 0}\n",
    "\n",
    "game = TicTacToe(learning_agent, random_agent2, display=False, width=m, height=m, win_length=k)\n",
    "\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"\\nOutcomes during learning:\")\n",
    "print(f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\")\n",
    "print(f\"Number of independent Q-matrix entries: {len(learning_agent.Q.get().keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from playGame import get_empty_positions\n",
    "\n",
    "def displayQ(Q, board):\n",
    "    actions = get_empty_positions(board)  # Assume this function returns indices of empty positions\n",
    "    Qs = {action: f\"{Q.get(tuple(board), action):.2f}\" for action in actions}  # Get Q-values, default to 0\n",
    "    board_size = int(len(board) ** 0.5)  # Assume square board\n",
    "\n",
    "    # Create a new board layout with Q-values embedded\n",
    "    Qboard = list(board)\n",
    "    for action, value in Qs.items():\n",
    "        Qboard[action] = value  # Replace empty spots with Q-values\n",
    "\n",
    "    cell_width = 5  # Padding for centering\n",
    "\n",
    "    # Format and display the board\n",
    "    for i in range(board_size):\n",
    "        row = Qboard[i * board_size:(i + 1) * board_size]\n",
    "        formatted_row = \" | \".join(str(cell).center(cell_width) for cell in row)\n",
    "        print(formatted_row)\n",
    "        if i < board_size - 1:\n",
    "            print(\"-\" * (board_size * cell_width + (board_size - 1) * 3))  # Line separator   \n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "def display_history(Q, history):\n",
    "    for i in range(len(history)):\n",
    "        board, action = history[i]\n",
    "        displayQ(Q, board)\n",
    "\n",
    "Q1 = learning_agent.Q\n",
    "historyX = paramsX['histories']\n",
    "Q2 = learning_agent2.Q\n",
    "historyO = paramsO['histories']\n",
    "for i in range(min((len(historyX), len(historyO)))):\n",
    "    board, action = historyO[i]\n",
    "    displayQ(Q2, board)\n",
    "    board, action = historyX[i]\n",
    "    displayQ(Q1, board)\n",
    "\n",
    "board, action = historyO[len(historyO) - 1]\n",
    "displayQ(Q1, board)\n",
    "\n",
    "# if max(len(historyX), len(historyO)) == len(historyX):\n",
    "#     board, action = historyX[len(historyX) - 1]\n",
    "#     displayQ(Q1, board)\n",
    "# else:\n",
    "#     board, action = historyO[len(historyO) - 1]\n",
    "#     displayQ(Q2, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def extract_values(dictionary):\n",
    "    \"\"\"Extract all values from a potentially nested dictionary.\"\"\"\n",
    "    values = []\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, dict):  # If the value is a dictionary, recurse\n",
    "            values.extend(extract_values(value))\n",
    "        else:\n",
    "            values.append(value)\n",
    "    return values\n",
    "\n",
    "def evaluate_and_plot_Q(learning_agent, player):\n",
    "    Q = learning_agent.Q\n",
    "    qMatrix = Q.get()\n",
    "    qValues = extract_values(qMatrix)\n",
    "    print(qValues)\n",
    "    print(f\"Total number of elements in Q for player {player}: {len(qValues)}\")\n",
    "    \n",
    "    mean_q = np.mean(qValues)\n",
    "    median_q = np.median(qValues)\n",
    "    std_q = np.std(qValues)\n",
    "    min_q = np.min(qValues)\n",
    "    max_q = np.max(qValues)\n",
    "\n",
    "    print(f\"Q-value Statistics for player {player}:\")\n",
    "    print(f\"Mean: {mean_q}\")\n",
    "    print(f\"Median: {median_q}\")\n",
    "    print(f\"Standard Deviation: {std_q}\")\n",
    "    print(f\"Minimum: {min_q}\")\n",
    "    print(f\"Maximum: {max_q}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(qValues, bins=20, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f\"Histogram of Q-values for player {player}\")\n",
    "    plt.xlabel(\"Q-value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "evaluate_and_plot_Q(learning_agent1, 'X')\n",
    "evaluate_and_plot_Q(learning_agent2, 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Generate sample data (Replace with your actual numerical data)\n",
    "# Example: A hypothetical 9-variable function f(x1, ..., x9) = x1^2 + x2*sin(x3) + exp(x4)*x5 - cos(x6)*x7 + x8*x9\n",
    "n_samples = 10000  # Number of samples\n",
    "inputs = np.random.uniform(-10, 10, size=(n_samples, 9))  # 9-dimensional input\n",
    "\n",
    "# Define the target function\n",
    "targets = (\n",
    "    inputs[:, 0] ** 2\n",
    "    + inputs[:, 1] * np.sin(inputs[:, 2])\n",
    "    + np.exp(inputs[:, 3]) * inputs[:, 4]\n",
    "    - np.cos(inputs[:, 5]) * inputs[:, 6]\n",
    "    + inputs[:, 7] * inputs[:, 8]\n",
    ")\n",
    "\n",
    "# Step 2: Define the Neural Network using TensorFlow\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(9,)),  # Hidden layer 1\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Hidden layer 2\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Hidden layer 3\n",
    "    tf.keras.layers.Dense(1)  # Output layer\n",
    "])\n",
    "model.summary()\n",
    "# Compile the model with MSE loss and Adam optimizer\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), \n",
    "              loss='mse')\n",
    "\n",
    "# Step 3: Train the model\n",
    "history = model.fit(inputs, targets, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 4: Plot training loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Evaluate the model and visualize predictions\n",
    "# Generate test data for evaluation\n",
    "test_inputs = np.random.uniform(-10, 10, size=(1000, 9))\n",
    "test_targets = (\n",
    "    test_inputs[:, 0] ** 2\n",
    "    + test_inputs[:, 1] * np.sin(test_inputs[:, 2])\n",
    "    + np.exp(test_inputs[:, 3]) * test_inputs[:, 4]\n",
    "    - np.cos(test_inputs[:, 5]) * test_inputs[:, 6]\n",
    "    + test_inputs[:, 7] * test_inputs[:, 8]\n",
    ")\n",
    "\n",
    "# Predict with the model\n",
    "predictions = model.predict(test_inputs)\n",
    "\n",
    "# Scatter plot of true vs predicted values\n",
    "plt.scatter(test_targets, predictions, alpha=0.6)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs. Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inputs) * 9)\n",
    "print(inputs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
