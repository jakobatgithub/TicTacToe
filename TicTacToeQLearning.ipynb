{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the games begin\n",
    "\n",
    "import copy\n",
    "\n",
    "from Agent import RandomAgent\n",
    "from QAgent import QLearningAgent\n",
    "from TicTacToe import TicTacToe\n",
    "\n",
    "params = {\n",
    "    \"nr_of_episodes\": 500000,  # number of episodes for training\n",
    "    \"rows\": 3,  # rows of the board, rows = cols\n",
    "    \"epsilon_start\": 0.15,  # initial exploration rate\n",
    "    \"epsilon_min\": 0.005,  # minimum exploration rate\n",
    "    \"alpha_start\": 0.1,  # initial learning rate\n",
    "    \"alpha_min\": 0.1,  # minimum learning rate\n",
    "    \"gamma\": 0.9,  # discount factor\n",
    "    \"switching\": False,  # switch between X and O\n",
    "    \"debug\": False,  # print debug messages\n",
    "    # Parameters for QAgent\n",
    "    \"lazy_evaluation\": True,  # use lazy evaluation\n",
    "    \"Q_initial_value\": 0.0,  # initial Q value\n",
    "    \"terminal_q_updates\": True,  # flag to switch between terminal and immediate Q updates\n",
    "    # Parameters for DeepQAgent\n",
    "    \"batch_size\": 32,  # batch size for deep learning\n",
    "    # 'target_update_frequency' : 250, # target network update frequency\n",
    "    \"target_update_frequency\": 150,  # target network update frequency\n",
    "    \"evaluation\": True,  # save data for evaluation\n",
    "    \"double_q_learning\": False,  # flag to switch on double Q-learnning\n",
    "}\n",
    "\n",
    "rows = 4\n",
    "win_length = 4\n",
    "nr_of_episodes = 750\n",
    "params[\"nr_of_episodes\"] = nr_of_episodes\n",
    "params[\"rows\"] = rows\n",
    "\n",
    "paramsX = copy.deepcopy(params)\n",
    "paramsO = copy.deepcopy(params)\n",
    "paramsX[\"player\"] = \"X\"\n",
    "paramsO[\"player\"] = \"O\"\n",
    "\n",
    "outcomes = {\"X\": 0, \"O\": 0, \"D\": 0, \"I\": 0}\n",
    "\n",
    "learning_agent1 = QLearningAgent(paramsX)\n",
    "learning_agent2 = QLearningAgent(paramsO)\n",
    "random_agent1 = RandomAgent(player=\"X\", switching=False)\n",
    "random_agent2 = RandomAgent(player=\"O\", switching=False)\n",
    "\n",
    "# game = TicTacToe(random_agent1, random_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "# game = TicTacToe(learning_agent1, random_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "game = TicTacToe(learning_agent1, learning_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "# game = TicTacToe(random_agent1, learning_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "# game = TicTacToe(random_agent1, learning_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "# game1 = TicTacToe(learning_agent1, random_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "# game2 = TicTacToe(random_agent1, learning_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "    # outcome1 = game1.play()\n",
    "    # outcome2 = game2.play()\n",
    "\n",
    "print(\"Outcomes during learning:\")\n",
    "print(\n",
    "    f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import QAgent_plays_against_QAgent, QAgent_plays_against_RandomAgent\n",
    "\n",
    "Q1 = learning_agent1.Q\n",
    "Q2 = learning_agent2.Q\n",
    "\n",
    "QAgent_plays_against_RandomAgent(Q1, \"X\", 2000, rows=rows, cols=rows, win_length=win_length)\n",
    "QAgent_plays_against_RandomAgent(Q2, \"O\", 2000, rows=rows, cols=rows, win_length=win_length)\n",
    "QAgent_plays_against_QAgent(Q1, \"X\", Q2, \"O\", 2000, rows=rows, cols=rows, win_length=win_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import plot_evaluation_data\n",
    "\n",
    "plot_evaluation_data(learning_agent1)\n",
    "plot_evaluation_data(learning_agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "\n",
    "# with open('SymmetricQ_optimalX.pkl', 'wb') as f:\n",
    "#     dill.dump(Q1.get(), f)\n",
    "\n",
    "# with open('SymmetricQ_optimalO.pkl', 'wb') as f:\n",
    "#     dill.dump(Q2.get(), f)\n",
    "\n",
    "# with open('TotallySymmetricQ_optimalX.pkl', 'wb') as f:\n",
    "#     dill.dump(Q1.get(), f)\n",
    "\n",
    "# with open('TotallySymmetricQ_optimalO.pkl', 'wb') as f:\n",
    "#     dill.dump(Q2.get(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from TicTacToe import TicTacToe\n",
    "\n",
    "get_empty_positions = TicTacToe.get_valid_actions_from_board\n",
    "\n",
    "\n",
    "def displayQ(Q, board):\n",
    "    actions = get_empty_positions(board)  # Assume this function returns indices of empty positions\n",
    "    Qs = {action: f\"{Q.get(tuple(board), action):.2f}\" for action in actions}  # Get Q-values, default to 0\n",
    "    board_size = int(len(board) ** 0.5)  # Assume square board\n",
    "\n",
    "    # Create a new board layout with Q-values embedded\n",
    "    Qboard = list(board)\n",
    "    for action, value in Qs.items():\n",
    "        Qboard[action] = value  # Replace empty spots with Q-values\n",
    "\n",
    "    cell_width = 5  # Padding for centering\n",
    "\n",
    "    # Format and display the board\n",
    "    for i in range(board_size):\n",
    "        row = Qboard[i * board_size : (i + 1) * board_size]\n",
    "        formatted_row = \" | \".join(str(cell).center(cell_width) for cell in row)\n",
    "        print(formatted_row)\n",
    "        if i < board_size - 1:\n",
    "            print(\"-\" * (board_size * cell_width + (board_size - 1) * 3))  # Line separator\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def display_history(Q, history):\n",
    "    for i in range(len(history)):\n",
    "        board, action = history[i]\n",
    "        displayQ(Q, board)\n",
    "\n",
    "\n",
    "# Q1 = learning_agent.Q\n",
    "# historyX = paramsX['histories']\n",
    "# Q2 = learning_agent2.Q\n",
    "# historyO = paramsO['histories']\n",
    "# for i in range(min((len(historyX), len(historyO)))):\n",
    "#     board, action = historyO[i]\n",
    "#     displayQ(Q2, board)\n",
    "#     board, action = historyX[i]\n",
    "#     displayQ(Q1, board)\n",
    "\n",
    "# board, action = historyO[len(historyO) - 1]\n",
    "# displayQ(Q1, board)\n",
    "\n",
    "# if max(len(historyX), len(historyO)) == len(historyX):\n",
    "#     board, action = historyX[len(historyX) - 1]\n",
    "#     displayQ(Q1, board)\n",
    "# else:\n",
    "#     board, action = historyO[len(historyO) - 1]\n",
    "#     displayQ(Q2, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def extract_values(dictionary):\n",
    "    \"\"\"Extract all values from a potentially nested dictionary.\"\"\"\n",
    "    values = []\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, dict):  # If the value is a dictionary, recurse\n",
    "            values.extend(extract_values(value))\n",
    "        else:\n",
    "            values.append(value)\n",
    "    return values\n",
    "\n",
    "\n",
    "def evaluate_and_plot_Q(learning_agent, player):\n",
    "    Q = learning_agent.Q\n",
    "    qMatrix = Q.get()\n",
    "    qValues = extract_values(qMatrix)\n",
    "    print(qValues)\n",
    "    print(f\"Total number of elements in Q for player {player}: {len(qValues)}\")\n",
    "\n",
    "    mean_q = np.mean(qValues)\n",
    "    median_q = np.median(qValues)\n",
    "    std_q = np.std(qValues)\n",
    "    min_q = np.min(qValues)\n",
    "    max_q = np.max(qValues)\n",
    "\n",
    "    print(f\"Q-value Statistics for player {player}:\")\n",
    "    print(f\"Mean: {mean_q}\")\n",
    "    print(f\"Median: {median_q}\")\n",
    "    print(f\"Standard Deviation: {std_q}\")\n",
    "    print(f\"Minimum: {min_q}\")\n",
    "    print(f\"Maximum: {max_q}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(qValues, bins=20, edgecolor=\"black\", alpha=0.7)\n",
    "    plt.title(f\"Histogram of Q-values for player {player}\")\n",
    "    plt.xlabel(\"Q-value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "evaluate_and_plot_Q(learning_agent1, \"X\")\n",
    "evaluate_and_plot_Q(learning_agent2, \"O\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
