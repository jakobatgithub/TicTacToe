{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the games begin\n",
    "\n",
    "import copy\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from TicTacToe.Agent import RandomAgent\n",
    "from TicTacToe.DeepQAgent import DeepQLearningAgent, DeepQPlayingAgent, ReplayBuffer\n",
    "from TicTacToe.Evaluation import evaluate_performance\n",
    "\n",
    "# os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "from TicTacToe.TicTacToe import TicTacToe\n",
    "\n",
    "params: dict[str, Any] = {\n",
    "    \"nr_of_episodes\": 500000,  # number of episodes for training\n",
    "    \"rows\": 3,  # rows of the board, rows = cols\n",
    "    \"epsilon_start\": 0.15,  # initial exploration rate\n",
    "    \"epsilon_min\": 0.05,  # minimum exploration rate\n",
    "    \"learning_rate\": 0.0001,  # learning rate\n",
    "    \"gamma\": 0.95,  # discount factor\n",
    "    \"switching\": True,  # switch between X and O\n",
    "    # Parameters for DeepQAgent\n",
    "    # \"batch_size\": 128,  # batch size for deep learning\n",
    "    \"batch_size\": 256,  # batch size for deep learning\n",
    "    \"target_update_frequency\": 20,  # target network update frequency\n",
    "    \"evaluation\": True,  # save data for evaluation\n",
    "    \"double_q_learning\": False,  # flag to switch on double Q-learnning\n",
    "    \"device\": \"cpu\",  # device to use, 'cpu' or 'mps' or 'cuda'\n",
    "    # \"replay_buffer_length\": 10000,  # replay buffer length\n",
    "    \"replay_buffer_length\": 20000,  # replay buffer length\n",
    "    \"wandb\": False,  # switch for logging with wand.ai\n",
    "    \"wandb_logging_frequency\": 100,  # wandb logging frequency\n",
    "    \"load_network\": False,  # file name of PyTorch network\n",
    "    \"shared_replay_buffer\": False,  # shared replay buffer\n",
    "}\n",
    "\n",
    "# rows = 6\n",
    "# win_length = 5\n",
    "# nr_of_episodes = 750000\n",
    "rows = 6\n",
    "win_length = 5\n",
    "nr_of_episodes = 25000\n",
    "evaluation_frequency = 500\n",
    "# shared_replay_buffer = ReplayBuffer(params[\"replay_buffer_length\"], rows**2, device=params[\"device\"])\n",
    "# params[\"shared_replay_buffer\"] = shared_replay_buffer\n",
    "params[\"nr_of_episodes\"] = nr_of_episodes\n",
    "params[\"rows\"] = rows\n",
    "\n",
    "paramsX = copy.deepcopy(params)\n",
    "paramsO = copy.deepcopy(params)\n",
    "paramsX[\"player\"] = \"X\"\n",
    "paramsO[\"player\"] = \"O\"\n",
    "paramsX[\"wandb\"] = True\n",
    "paramsO[\"wandb\"] = False\n",
    "\n",
    "outcomes = {\"X\": 0, \"O\": 0, \"D\": 0}\n",
    "\n",
    "learning_agent1 = DeepQLearningAgent(paramsX)\n",
    "learning_agent2 = DeepQLearningAgent(paramsO)\n",
    "# random_agent = RandomAgent(player=\"O\", switching=True)\n",
    "\n",
    "game = TicTacToe(learning_agent1, learning_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "\n",
    "try:\n",
    "    for episode in range(nr_of_episodes):\n",
    "        outcome = game.play()\n",
    "        if outcome is not None:\n",
    "            outcomes[outcome] += 1\n",
    "\n",
    "        if episode % evaluation_frequency == 0:\n",
    "            evaluate_performance(learning_agent1, learning_agent2, rows=rows, win_length=win_length, wandb_logging=True)\n",
    "\n",
    "    print(\"Outcomes during learning:\")\n",
    "    print(\n",
    "        f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\"\n",
    "    )\n",
    "\n",
    "finally:\n",
    "    torch.save(learning_agent1.q_network, f\"../models/q_network_{rows}x{rows}x{win_length}_X.pth\")\n",
    "    torch.save(learning_agent2.q_network, f\"../models/q_network_{rows}x{rows}x{win_length}_O.pth\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "q_network1 = learning_agent1.q_network\n",
    "\n",
    "board1 = np.array([\"X\", \"O\", \" \", \" \", \" \", \" \", \" \", \" \", \" \"])\n",
    "action1 = 2\n",
    "\n",
    "board2 = np.array([\" \", \"O\", \"X\", \" \", \" \", \" \", \" \", \" \", \" \"])\n",
    "action2 = 0\n",
    "\n",
    "permutations, inverse_permutations = learning_agent1.generate_permutations([lambda x: x, lambda x: np.rot90(x, k=3)], 3)\n",
    "\n",
    "state1 = torch.Tensor(learning_agent1.board_to_state(board1)[0])\n",
    "state2 = torch.Tensor(learning_agent1.board_to_state(board2)[0])\n",
    "\n",
    "board1 = np.array([\"X\", \"O\", \"X\", \" \", \" \", \" \", \" \", \"O\", \" \"])\n",
    "action1 = 1\n",
    "state1 = torch.Tensor(learning_agent1.board_to_state(board1)[0])\n",
    "\n",
    "print(q_network1(state1))\n",
    "print(q_network1(state1[permutations[1]]))\n",
    "print(q_network1(state1)[permutations[1]])\n",
    "\n",
    "print(torch.sum((q_network1(state1) - q_network1(state1[permutations[1]])) ** 2) / torch.sum((q_network1(state1)) ** 2))\n",
    "print(\n",
    "    torch.sum((q_network1(state1)[permutations[1]] - q_network1(state1[permutations[1]])) ** 2)\n",
    "    / torch.sum((q_network1(state1)[permutations[1]]) ** 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DeepQLearningAgent(paramsX)\n",
    "q_network1 = agent.q_network\n",
    "\n",
    "board1 = np.array([\"X\", \"O\", \"X\", \" \", \" \", \" \", \" \", \" \", \" \"])\n",
    "action1 = 1\n",
    "state1 = torch.Tensor(learning_agent1.board_to_state(board1)[0])\n",
    "\n",
    "print(q_network1(state1))\n",
    "print(q_network1(state1[permutations[1]]))\n",
    "print(q_network1(state1)[permutations[1]])\n",
    "\n",
    "print(torch.sum((q_network1(state1) - q_network1(state1[permutations[1]])) ** 2) / torch.sum((q_network1(state1)) ** 2))\n",
    "print(\n",
    "    torch.sum((q_network1(state1)[permutations[1]] - q_network1(state1[permutations[1]])) ** 2)\n",
    "    / torch.sum((q_network1(state1)[permutations[1]]) ** 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network1 = learning_agent1.q_network\n",
    "playing_agent1 = DeepQPlayingAgent(q_network1, player=\"X\", switching=False)\n",
    "random_agent2 = RandomAgent(player=\"O\", switching=False)\n",
    "\n",
    "game = TicTacToe(playing_agent1, random_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "nr_of_episodes = 1000\n",
    "outcomes = {\"X\": 0, \"O\": 0, \"D\": 0}\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during playing:\")\n",
    "print(\n",
    "    f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\"\n",
    ")\n",
    "\n",
    "q_network2 = learning_agent2.q_network\n",
    "playing_agent2 = DeepQPlayingAgent(q_network2, player=\"O\", switching=False)\n",
    "random_agent1 = RandomAgent(player=\"X\", switching=False)\n",
    "\n",
    "game = TicTacToe(random_agent1, playing_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "nr_of_episodes = 1000\n",
    "outcomes = {\"X\": 0, \"O\": 0, \"D\": 0}\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during playing:\")\n",
    "print(\n",
    "    f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\"\n",
    ")\n",
    "\n",
    "game = TicTacToe(playing_agent1, playing_agent2, display=None, rows=rows, cols=rows, win_length=win_length)\n",
    "nr_of_episodes = 1000\n",
    "outcomes = {\"X\": 0, \"O\": 0, \"D\": 0}\n",
    "for episode in range(nr_of_episodes):\n",
    "    outcome = game.play()\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(\"Outcomes during playing:\")\n",
    "print(\n",
    "    f\"X wins: {outcomes['X']/nr_of_episodes}, O wins: {outcomes['O']/nr_of_episodes}, draws: {outcomes['D']/nr_of_episodes}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# torch.save(q_network1, 'models/q_network_4x4x4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import plot_evaluation_data, plot_valid_actions\n",
    "\n",
    "plot_evaluation_data(learning_agent1)\n",
    "plot_evaluation_data(learning_agent2)\n",
    "\n",
    "plot_valid_actions(learning_agent1)\n",
    "plot_valid_actions(learning_agent2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
